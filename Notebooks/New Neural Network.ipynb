{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as stats\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "from category_encoders import OrdinalEncoder, TargetEncoder\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    errors = abs(predictions - y_test)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    roc = roc_auc_score(y_test, predictions)\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%'.format(accuracy))\n",
    "    print(f'AUC = {roc}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/training_set_features.csv', index_col='respondent_id')\n",
    "test = pd.read_csv('../Data/test_set_features.csv', index_col ='respondent_id')\n",
    "labels = pd.read_csv('../Data/training_set_labels.csv', index_col='respondent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['age_group'] == '65+ Years') & (train['employment_status'].isnull()), 'employment_status'] = 'Not in Labor Force'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(train.select_dtypes('number').columns)\n",
    "\n",
    "cat_cols = [\n",
    "    'race',\n",
    "    'sex',\n",
    "    'marital_status',\n",
    "    'rent_or_own',\n",
    "    'hhs_geo_region',\n",
    "    'census_msa',\n",
    "    'employment_industry',\n",
    "    'employment_occupation'\n",
    "]\n",
    "\n",
    "ord_cols = [\n",
    "    'age_group',\n",
    "    'education',\n",
    "    'income_poverty',\n",
    "    'employment_status'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Impute Train\n",
    "for col in num_cols:\n",
    "    train[col] = train[col].fillna(value=-1)\n",
    "    test[col] = test[col].fillna(value=-1)\n",
    "\n",
    "for col in (cat_cols + ord_cols):\n",
    "    train[col] = train[col].fillna(value='None')\n",
    "    test[col] = test[col].fillna(value='None')\n",
    "test_labels = labels.copy()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_group'] = train['age_group'].map({\n",
    "    '18 - 34 Years': 1,\n",
    "    '35 - 44 Years': 2,\n",
    "    '45 - 54 Years': 3,\n",
    "    '55 - 64 Years': 4,\n",
    "    '65+ Years': 5\n",
    "})\n",
    "\n",
    "train['education'] = train['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "train['income_poverty'] = train['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "train['employment_status'] = train['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['education'] = test['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "test['income_poverty'] = test['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "test['employment_status'] = test['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_group [4 2 1 5 3]\n",
      "education [ 1  2  4  3 -1]\n",
      "income_poverty [ 1  2  3 -1]\n",
      "employment_status [ 3  2  1 -1]\n"
     ]
    }
   ],
   "source": [
    "for x in train[ord_cols].columns:\n",
    "    print(x, train[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols_names = [all_cols[x] for x in best_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_labels = labels[['h1n1_vaccine']]\n",
    "seas_labels = labels[['seasonal_vaccine']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H1N1 Balancing\n",
    "yeses = h1n1_labels[h1n1_labels['h1n1_vaccine'] == 1]\n",
    "len_of_yes = len(yeses)\n",
    "nos = h1n1_labels[h1n1_labels['h1n1_vaccine'] == 0].sample(len_of_yes, random_state=42)\n",
    "\n",
    "indices = np.concatenate((yeses.index.values, nos.index.values))\n",
    "h1n1_labels_balanced = h1n1_labels.iloc[indices, :]\n",
    "h1n1_train_balanced = train.iloc[indices, :]\n",
    "\n",
    "# enc = OneHotEncoder(categories='auto')\n",
    "# h1n1_labels_balanced_arr = np.array(h1n1_labels_balanced['h1n1_vaccine']).reshape(-1,1)\n",
    "# h1n1_labels_trans = enc.fit_transform(h1n1_labels_balanced_arr).toarray()\n",
    "# h1n1_test_trans = enc.transform(np.array(h1n1_labels['h1n1_vaccine']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal Balancing\n",
    "yeses = seas_labels[seas_labels['seasonal_vaccine'] == 1]\n",
    "len_of_yes = len(yeses)\n",
    "nos = seas_labels[seas_labels['seasonal_vaccine'] == 0].sample(len_of_yes, random_state=42)\n",
    "\n",
    "indices = np.concatenate((yeses.index.values, nos.index.values))\n",
    "seas_labels_balanced = seas_labels.iloc[indices, :]\n",
    "seas_train_balanced = train.iloc[indices, :]\n",
    "\n",
    "# enc = OneHotEncoder(categories='auto')\n",
    "# seas_labels_balanced_arr = np.array(seas_labels_balanced['seasonal_vaccine']).reshape(-1,1)\n",
    "# seas_labels_trans = enc.fit_transform(seas_labels_balanced_arr).toarray()\n",
    "# seas_test_trans = enc.transform(np.array(seas_labels['seasonal_vaccine']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_h1n1 = ColumnTransformer([('scaler', StandardScaler(), num_cols),\n",
    "                       ('cat', TargetEncoder(cols=cat_cols, smoothing=100, min_samples_leaf=10), cat_cols)]\n",
    "                       , remainder='passthrough')\n",
    "\n",
    "ct_seas = ColumnTransformer([('scaler', StandardScaler(), num_cols),\n",
    "                       ('cat', TargetEncoder(cols=cat_cols, smoothing=100, min_samples_leaf=10), cat_cols)]\n",
    "                       , remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_train_trans = ct_h1n1.fit_transform(h1n1_train_balanced, h1n1_labels_balanced)\n",
    "seas_train_trans = ct_seas.fit_transform(seas_train_balanced, seas_labels_balanced)\n",
    "test_h1n1 = ct_h1n1.transform(test)\n",
    "test_seas = ct_seas.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_features = train.sample(1500, random_state=42)\n",
    "true_indices = true_features.index.values\n",
    "true_labels = labels.iloc[true_indices,:]\n",
    "\n",
    "true_features = ct_seas.transform(true_features)\n",
    "true_labels_rf = true_labels.copy()\n",
    "true_labels = np.asarray(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [0, 0],\n",
       "       ...,\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1N1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>respondent_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14227</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20168</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11348 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               h1n1_vaccine\n",
       "respondent_id              \n",
       "7                         1\n",
       "10                        1\n",
       "11                        1\n",
       "16                        1\n",
       "26                        1\n",
       "...                     ...\n",
       "14227                     0\n",
       "1668                      0\n",
       "20168                     0\n",
       "968                       0\n",
       "12066                     0\n",
       "\n",
       "[11348 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = h1n1_train_trans\n",
    "y = h1n1_labels_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H1N1 Model\n",
    "# model_h1n1 = RandomForestRegressor(n_estimators=1200,\n",
    "#                                min_samples_split=2,\n",
    "#                                min_samples_leaf=3,\n",
    "#                                max_features='sqrt',\n",
    "#                                max_depth=20,\n",
    "#                                bootstrap=True)\n",
    "model_h1n1 = CatBoostRegressor(n_estimators=150)\n",
    "model_h1n1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base Model\n",
    "base_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model_h1n1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_h1n1 = model_h1n1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h1n1_train_trans\n",
    "y = h1n1_labels_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_mc = ModelCheckpoint('..Models/h1n1_best_model.h5', monitor='val_auc', mode='max', verbose=0, save_best_only=True)\n",
    "\n",
    "model_h1n1 = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='relu', input_dim=31),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(800, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(250, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h1n1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.AUC(from_logits=False), 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "159/159 [==============================] - 2s 8ms/step - loss: 0.5208 - auc_9: 0.8191 - accuracy: 0.7452 - val_loss: 0.4914 - val_auc_9: 0.8488 - val_accuracy: 0.7518\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 2/150\n",
      "159/159 [==============================] - 1s 7ms/step - loss: 0.4747 - auc_9: 0.8538 - accuracy: 0.7841 - val_loss: 0.4762 - val_auc_9: 0.8542 - val_accuracy: 0.7742\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 3/150\n",
      "159/159 [==============================] - 1s 6ms/step - loss: 0.4607 - auc_9: 0.8629 - accuracy: 0.7871 - val_loss: 0.4739 - val_auc_9: 0.8568 - val_accuracy: 0.7700\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 4/150\n",
      "159/159 [==============================] - 1s 6ms/step - loss: 0.4458 - auc_9: 0.8732 - accuracy: 0.7973 - val_loss: 0.5042 - val_auc_9: 0.8482 - val_accuracy: 0.7624\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 5/150\n",
      "159/159 [==============================] - 1s 7ms/step - loss: 0.4321 - auc_9: 0.8810 - accuracy: 0.7992 - val_loss: 0.5017 - val_auc_9: 0.8432 - val_accuracy: 0.7604\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 6/150\n",
      "159/159 [==============================] - 1s 7ms/step - loss: 0.4166 - auc_9: 0.8896 - accuracy: 0.8110 - val_loss: 0.5013 - val_auc_9: 0.8421 - val_accuracy: 0.7539\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 7/150\n",
      "159/159 [==============================] - 1s 7ms/step - loss: 0.3997 - auc_9: 0.8992 - accuracy: 0.8183 - val_loss: 0.5062 - val_auc_9: 0.8392 - val_accuracy: 0.7604\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 8/150\n",
      "159/159 [==============================] - 1s 7ms/step - loss: 0.3817 - auc_9: 0.9084 - accuracy: 0.8288 - val_loss: 0.5228 - val_auc_9: 0.8349 - val_accuracy: 0.7533\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n"
     ]
    }
   ],
   "source": [
    "EarlyStopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model_h1n1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[h1n1_mc, EarlyStopping],\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567688992454382"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_h1n1 = load_model('..Models/h1n1_best_model.h5')\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test)\n",
    "roc_auc_score(y_test, y_predicted_h1n1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = seas_train_trans\n",
    "y = seas_labels_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=False, max_depth=20, max_features='sqrt',\n",
       "                      min_samples_leaf=4, n_estimators=800)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seasonal Model\n",
    "model_seas = RandomForestRegressor(n_estimators=800,\n",
    "                                    min_samples_split=2,\n",
    "                                    min_samples_leaf=4,\n",
    "                                    max_features='sqrt',\n",
    "                                    max_depth=20,bootstrap=False)\n",
    "#model_seas = CatBoostRegressor(n_estimators=150)\n",
    "model_seas.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Base Model\n",
    "base_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 1: given 7461",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-196c0e1db0c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-e8fb442df30f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_frame_arith_method_with_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    639\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 7461"
     ]
    }
   ],
   "source": [
    "evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to coerce to Series, length must be 1: given 7461",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-8bed37d5ddad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_seas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-65-e8fb442df30f>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, X_test, y_test)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mmape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(self, other, axis, level, fill_value)\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_frame_arith_method_with_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_align_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36m_align_method_FRAME\u001b[1;34m(left, right, axis)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m             \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py\u001b[0m in \u001b[0;36mto_series\u001b[1;34m(right)\u001b[0m\n\u001b[0;32m    636\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    639\u001b[0m                     \u001b[0mmsg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgiven_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to coerce to Series, length must be 1: given 7461"
     ]
    }
   ],
   "source": [
    "evaluate(model_seas, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_seas = model_seas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = np.vstack((y_predicted_h1n1, y_predicted_seas)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_predicted_seas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = seas_train_trans\n",
    "y = seas_labels_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "seas_mc = ModelCheckpoint('..Models/seas_best_model.h5', monitor='val_auc_2', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model_seas = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation='relu', input_dim=31),\n",
    "    keras.layers.LeakyReLU(500),\n",
    "    keras.layers.LeakyReLU(600),\n",
    "    keras.layers.LeakyReLU(820),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seas.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.AUC(from_logits=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "349/349 [==============================] - 1s 2ms/step - loss: 0.5197 - auc_10: 0.8214 - val_loss: 0.4866 - val_auc_10: 0.8477\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 2/150\n",
      "349/349 [==============================] - ETA: 0s - loss: 0.4926 - auc_10: 0.84 - 0s 1ms/step - loss: 0.4914 - auc_10: 0.8431 - val_loss: 0.4858 - val_auc_10: 0.8504\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 3/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4817 - auc_10: 0.8499 - val_loss: 0.4832 - val_auc_10: 0.8516\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 4/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4739 - auc_10: 0.8551 - val_loss: 0.4813 - val_auc_10: 0.8511\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 5/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4673 - auc_10: 0.8599 - val_loss: 0.4831 - val_auc_10: 0.8520\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 6/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4617 - auc_10: 0.8635 - val_loss: 0.4834 - val_auc_10: 0.8497\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 7/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4557 - auc_10: 0.8671 - val_loss: 0.4831 - val_auc_10: 0.8531\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 8/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4512 - auc_10: 0.8703 - val_loss: 0.4870 - val_auc_10: 0.8477\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 9/150\n",
      "349/349 [==============================] - 0s 1ms/step - loss: 0.4439 - auc_10: 0.8744 - val_loss: 0.4839 - val_auc_10: 0.8499\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n"
     ]
    }
   ],
   "source": [
    "history = model_seas.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[seas_mc, EarlyStopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8510831972210825"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_seas = load_model('..Models/seas_best_model.h5')\n",
    "\n",
    "y_predicted_seas = model_seas.predict(X_test)\n",
    "roc_auc_score(y_test, y_predicted_seas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Data Test - Random Forest Network\n",
    "X_test = true_features\n",
    "y_test = true_labels_rf.to_numpy()\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test).reshape(-1,1)\n",
    "y_predicted_seas = model_seas.predict(X_test).reshape(-1,1)\n",
    "\n",
    "y_predicted = np.concatenate((y_predicted_h1n1, y_predicted_seas), axis=1)\n",
    "roc_auc_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8794472852265591"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full Data Test - Neural Network\n",
    "X_test = true_features\n",
    "y_test = true_labels\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test)\n",
    "y_predicted_seas = model_seas.predict(X_test)\n",
    "\n",
    "y_predicted = np.concatenate((y_predicted_h1n1, y_predicted_seas), axis=1)\n",
    "roc_auc_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48894233, 0.36433274],\n",
       "       [0.4735105 , 0.14668101],\n",
       "       [0.4070636 , 0.91004425],\n",
       "       ...,\n",
       "       [0.5524947 , 0.20513529],\n",
       "       [0.17454535, 0.38615042],\n",
       "       [0.0442811 , 0.08562094]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../Data/test_set_features.csv')\n",
    "full_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(test.select_dtypes('number').columns)\n",
    "\n",
    "cat_cols = [\n",
    "    'race',\n",
    "    'sex',\n",
    "    'marital_status',\n",
    "    'rent_or_own',\n",
    "    'hhs_geo_region',\n",
    "    'census_msa',\n",
    "    'employment_industry',\n",
    "    'employment_occupation'\n",
    "]\n",
    "\n",
    "ord_cols = [\n",
    "    'age_group',\n",
    "    'education',\n",
    "    'income_poverty',\n",
    "    'employment_status'\n",
    "]\n",
    "\n",
    "\n",
    "#Impute Test\n",
    "for col in num_cols:\n",
    "    test[col] = test[col].fillna(value=-1)\n",
    "\n",
    "\n",
    "for col in (cat_cols+ord_cols):\n",
    "    test[col] = test[col].fillna(value='None')\n",
    "\n",
    "    \n",
    "test['age_group'] = test['age_group'].map({\n",
    "    '18 - 34 Years': 1,\n",
    "    '35 - 44 Years': 2,\n",
    "    '45 - 54 Years': 3,\n",
    "    '55 - 64 Years': 4,\n",
    "    '65+ Years': 5\n",
    "})\n",
    "    \n",
    "test['education'] = test['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "test['income_poverty'] = test['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "test['employment_status'] = test['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_h1n1 = ct_h1n1.transform(test)\n",
    "test_seas = ct_seas.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1 = model_h1n1.predict(test_h1n1)\n",
    "y_seas = model_seas.predict(test_seas)\n",
    "\n",
    "y_comb = np.concatenate((y_h1n1, y_seas), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32304165, 0.26871872],\n",
       "       [0.15819561, 0.13347876],\n",
       "       [0.46876505, 0.80193985],\n",
       "       ...,\n",
       "       [0.3317409 , 0.21633819],\n",
       "       [0.08595854, 0.41861343],\n",
       "       [0.8881855 , 0.7818744 ]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(y_comb, columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "\n",
    "submission = pd.concat([full_test, results], axis=1)\n",
    "submission = submission[['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "\n",
    "submission.to_csv(f'../Submissions/Neural Network Submission {today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
