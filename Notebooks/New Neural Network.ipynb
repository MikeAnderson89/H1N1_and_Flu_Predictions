{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats as stats\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from datetime import datetime\n",
    "from category_encoders import OrdinalEncoder\n",
    "from catboost import CatBoostClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    errors = abs(predictions - y_test)\n",
    "    mape = 100 * np.mean(errors / y_test)\n",
    "    accuracy = 100 - mape\n",
    "    roc = roc_auc_score(y_test, predictions)\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%'.format(accuracy))\n",
    "    print(f'AUC = {roc}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/training_set_features.csv', index_col='respondent_id')\n",
    "test = pd.read_csv('../Data/test_set_features.csv', index_col ='respondent_id')\n",
    "labels = pd.read_csv('../Data/training_set_labels.csv', index_col='respondent_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['age_group'] == '65+ Years') & (train['employment_status'].isnull()), 'employment_status'] = 'Not in Labor Force'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(train.select_dtypes('number').columns)\n",
    "\n",
    "cat_cols = [\n",
    "    'race',\n",
    "    'sex',\n",
    "    'marital_status',\n",
    "    'rent_or_own',\n",
    "    'hhs_geo_region',\n",
    "    'census_msa',\n",
    "    'employment_industry',\n",
    "    'employment_occupation'\n",
    "]\n",
    "\n",
    "ord_cols = [\n",
    "    'age_group',\n",
    "    'education',\n",
    "    'income_poverty',\n",
    "    'employment_status'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Impute Train\n",
    "for col in num_cols:\n",
    "    train[col] = train[col].fillna(value=-1)\n",
    "    test[col] = test[col].fillna(value=-1)\n",
    "\n",
    "for col in (cat_cols + ord_cols):\n",
    "    train[col] = train[col].fillna(value='None')\n",
    "    test[col] = test[col].fillna(value='None')\n",
    "test_labels = labels.copy()    \n",
    "labels['h1n1_vaccine'] = labels['h1n1_vaccine'].map({0: 'No', 1: 'Yes'})\n",
    "labels['seasonal_vaccine'] = labels['seasonal_vaccine'].map({0: 'No', 1: 'Yes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age_group'] = train['age_group'].map({\n",
    "    '18 - 34 Years': 1,\n",
    "    '35 - 44 Years': 2,\n",
    "    '45 - 54 Years': 3,\n",
    "    '55 - 64 Years': 4,\n",
    "    '65+ Years': 5\n",
    "})\n",
    "\n",
    "train['education'] = train['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "train['income_poverty'] = train['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "train['employment_status'] = train['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test['education'] = test['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "test['income_poverty'] = test['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "test['employment_status'] = test['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_group [4 2 1 5 3]\n",
      "education [ 1  2  4  3 -1]\n",
      "income_poverty [ 1  2  3 -1]\n",
      "employment_status [ 3  2  1 -1]\n"
     ]
    }
   ],
   "source": [
    "for x in train[ord_cols].columns:\n",
    "    print(x, train[x].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = train.columns\n",
    "best_cols = [2, 3, 6, 7, 10, 12, 13, 14, 16, 21, 22, 25, 26, 27, 28, 29, 30, 31, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cols_names = [all_cols[x] for x in best_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = list(set(num_cols + best_cols_names))\n",
    "train = train[used_cols]\n",
    "test = test[used_cols]\n",
    "\n",
    "h1n1_labels = labels[['h1n1_vaccine']]\n",
    "seas_labels = labels[['seasonal_vaccine']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H1N1 Balancing\n",
    "yeses = h1n1_labels[h1n1_labels['h1n1_vaccine'] == 'Yes']\n",
    "len_of_yes = len(yeses)\n",
    "nos = h1n1_labels[h1n1_labels['h1n1_vaccine'] == 'No'].sample(len_of_yes, random_state=42)\n",
    "\n",
    "indices = np.concatenate((yeses.index.values, nos.index.values))\n",
    "h1n1_labels_balanced = h1n1_labels.iloc[indices, :]\n",
    "h1n1_train_balanced = train.iloc[indices, :]\n",
    "\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "h1n1_labels_balanced_arr = np.array(h1n1_labels_balanced['h1n1_vaccine']).reshape(-1,1)\n",
    "h1n1_labels_trans = enc.fit_transform(h1n1_labels_balanced_arr).toarray()\n",
    "h1n1_test_trans = enc.transform(np.array(h1n1_labels['h1n1_vaccine']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal Balancing\n",
    "yeses = seas_labels[seas_labels['seasonal_vaccine'] == 'Yes']\n",
    "len_of_yes = len(yeses)\n",
    "nos = seas_labels[seas_labels['seasonal_vaccine'] == 'No'].sample(len_of_yes, random_state=42)\n",
    "\n",
    "indices = np.concatenate((yeses.index.values, nos.index.values))\n",
    "seas_labels_balanced = seas_labels.iloc[indices, :]\n",
    "seas_train_balanced = train.iloc[indices, :]\n",
    "\n",
    "enc = OneHotEncoder(categories='auto')\n",
    "seas_labels_balanced_arr = np.array(seas_labels_balanced['seasonal_vaccine']).reshape(-1,1)\n",
    "seas_labels_trans = enc.fit_transform(seas_labels_balanced_arr).toarray()\n",
    "seas_test_trans = enc.transform(np.array(seas_labels['seasonal_vaccine']).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = train.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([('scaler', StandardScaler(), num_cols),\n",
    "                       ('cat', OneHotEncoder(), cat_cols)]\n",
    "                       , remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_train_trans = ct.fit_transform(h1n1_train_balanced)\n",
    "seas_train_trans = ct.fit_transform(seas_train_balanced)\n",
    "test = ct.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_features = train.sample(1500, random_state=42)\n",
    "true_indices = true_features.index.values\n",
    "true_labels = labels.iloc[true_indices,:]\n",
    "\n",
    "true_features = ct.fit_transform(true_features)\n",
    "true_labels['h1n1_vaccine'] = true_labels['h1n1_vaccine'].map({'Yes': 1, 'No': 0})\n",
    "true_labels['seasonal_vaccine'] = true_labels['seasonal_vaccine'].map({'Yes': 1, 'No': 0})\n",
    "true_labels_rf = true_labels.copy()\n",
    "true_labels = np.asarray(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1N1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h1n1_train_trans\n",
    "y = h1n1_labels_balanced['h1n1_vaccine'].map({'Yes': 1, 'No': 0}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=20, max_features='sqrt', min_samples_leaf=3,\n",
       "                      n_estimators=1200)"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#H1N1 Model\n",
    "model_h1n1 = RandomForestRegressor(n_estimators=1200,\n",
    "                               min_samples_split=2,\n",
    "                               min_samples_leaf=3,\n",
    "                               max_features='sqrt',\n",
    "                               max_depth=20,\n",
    "                               bootstrap=True)\n",
    "#model_h1n1 = RandomForestClassifier()\n",
    "model_h1n1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 660,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Base Model\n",
    "base_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.2511 degrees\n",
      "Accuracy = nan%\n",
      "AUC = 0.7500653994925414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.3345 degrees\n",
      "Accuracy = -inf%\n",
      "AUC = 0.8569618191205337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_h1n1, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_h1n1 = model_h1n1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = h1n1_train_trans\n",
    "y = h1n1_labels_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1n1_mc = ModelCheckpoint('..Models/h1n1_best_model.h5', monitor='val_auc', mode='max', verbose=0, save_best_only=True)\n",
    "\n",
    "model_h1n1 = keras.Sequential([\n",
    "    keras.layers.Dense(200, activation='sigmoid', input_dim=46),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(800, activation='relu'),\n",
    "    keras.layers.Dense(500, activation='relu'),\n",
    "    keras.layers.Dense(250, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_h1n1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.AUC(from_logits=False), 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "159/159 [==============================] - 5s 27ms/step - loss: 0.5396 - auc_43: 0.8032 - accuracy: 0.7269 - val_loss: 0.4845 - val_auc_43: 0.8481 - val_accuracy: 0.7651\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 2/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4910 - auc_43: 0.8437 - accuracy: 0.7675 - val_loss: 0.4858 - val_auc_43: 0.8516 - val_accuracy: 0.7715\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 3/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4820 - auc_43: 0.8504 - accuracy: 0.7743 - val_loss: 0.4817 - val_auc_43: 0.8502 - val_accuracy: 0.7671\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 4/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4730 - auc_43: 0.8559 - accuracy: 0.7818 - val_loss: 0.4823 - val_auc_43: 0.8503 - val_accuracy: 0.7756\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 5/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4715 - auc_43: 0.8569 - accuracy: 0.7778 - val_loss: 0.4980 - val_auc_43: 0.8385 - val_accuracy: 0.7659\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 6/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4719 - auc_43: 0.8564 - accuracy: 0.7803 - val_loss: 0.4872 - val_auc_43: 0.8480 - val_accuracy: 0.7562\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 7/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4664 - auc_43: 0.8605 - accuracy: 0.7803 - val_loss: 0.4963 - val_auc_43: 0.8411 - val_accuracy: 0.7733\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n",
      "Epoch 8/150\n",
      "159/159 [==============================] - 4s 25ms/step - loss: 0.4608 - auc_43: 0.8638 - accuracy: 0.7867 - val_loss: 0.4859 - val_auc_43: 0.8486 - val_accuracy: 0.7744\n",
      "WARNING:tensorflow:Can save best model only with val_auc available, skipping.\n"
     ]
    }
   ],
   "source": [
    "EarlyStopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=5, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model_h1n1.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[h1n1_mc, EarlyStopping],\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8538729441436566"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_h1n1 = load_model('..Models/h1n1_best_model.h5')\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test)\n",
    "roc_auc_score(y_test, y_predicted_h1n1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = seas_train_trans\n",
    "y = seas_labels_balanced['seasonal_vaccine'].map({'Yes': 1, 'No': 0}).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=False, max_depth=20, max_features='sqrt',\n",
       "                      min_samples_leaf=4, n_estimators=800)"
      ]
     },
     "execution_count": 689,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Seasonal Model\n",
    "model_seas = RandomForestRegressor(n_estimators=800,\n",
    "                              min_samples_split=2,\n",
    "                              min_samples_leaf=4,\n",
    "                              max_features='sqrt',\n",
    "                              max_depth=20,\n",
    "                              bootstrap=False)\n",
    "\n",
    "model_seas.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Base Model\n",
    "base_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.3240 degrees\n",
      "Accuracy = nan%\n",
      "AUC = 0.8138888389870517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(base_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.3313 degrees\n",
      "Accuracy = -inf%\n",
      "AUC = 0.8560692889802611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model_seas, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_seas = model_seas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 7461",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-694-31a9b405fea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predicted_h1n1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_predicted_seas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 1 and the array at index 1 has size 7461"
     ]
    }
   ],
   "source": [
    "y_predicted = np.vstack((y_predicted_h1n1, y_predicted_seas)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8560692889802611"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_predicted_seas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = seas_train_trans\n",
    "y = seas_labels_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "seas_mc = ModelCheckpoint('..Models/seas_best_model.h5', monitor='val_auc_2', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "\n",
    "model_seas = keras.Sequential([\n",
    "    keras.layers.Dense(100, activation='sigmoid', input_dim=46),\n",
    "    keras.layers.LeakyReLU(500),\n",
    "    keras.layers.LeakyReLU(600),\n",
    "    keras.layers.LeakyReLU(820),\n",
    "    keras.layers.Dense(200, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seas.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              metrics=[tf.keras.metrics.AUC(from_logits=False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "349/349 [==============================] - 2s 6ms/step - loss: 0.5367 - auc_44: 0.8097 - val_loss: 0.4968 - val_auc_44: 0.8393\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 2/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4994 - auc_44: 0.8374 - val_loss: 0.4934 - val_auc_44: 0.8418\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 3/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4945 - auc_44: 0.8411 - val_loss: 0.4824 - val_auc_44: 0.8510\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 4/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4925 - auc_44: 0.8426 - val_loss: 0.4799 - val_auc_44: 0.8523\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 5/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4884 - auc_44: 0.8455 - val_loss: 0.4774 - val_auc_44: 0.8534\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 6/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4855 - auc_44: 0.8475 - val_loss: 0.4777 - val_auc_44: 0.8531\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 7/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4831 - auc_44: 0.8491 - val_loss: 0.4746 - val_auc_44: 0.8554\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 8/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4812 - auc_44: 0.8505 - val_loss: 0.4827 - val_auc_44: 0.8501\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 9/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4794 - auc_44: 0.8516 - val_loss: 0.4743 - val_auc_44: 0.8554\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 10/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4774 - auc_44: 0.8531 - val_loss: 0.4728 - val_auc_44: 0.8568\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 11/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4771 - auc_44: 0.8532 - val_loss: 0.4778 - val_auc_44: 0.8538\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 12/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4743 - auc_44: 0.8553 - val_loss: 0.4709 - val_auc_44: 0.8580\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 13/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4721 - auc_44: 0.8567 - val_loss: 0.4715 - val_auc_44: 0.8581\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 14/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4706 - auc_44: 0.8577 - val_loss: 0.4729 - val_auc_44: 0.8574\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 15/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4700 - auc_44: 0.8581 - val_loss: 0.4740 - val_auc_44: 0.8555\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 16/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4680 - auc_44: 0.8593 - val_loss: 0.4720 - val_auc_44: 0.8571\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n",
      "Epoch 17/150\n",
      "349/349 [==============================] - 2s 5ms/step - loss: 0.4662 - auc_44: 0.8604 - val_loss: 0.4724 - val_auc_44: 0.8567\n",
      "WARNING:tensorflow:Can save best model only with val_auc_2 available, skipping.\n"
     ]
    }
   ],
   "source": [
    "history = model_seas.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=50,\n",
    "    epochs=150,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[seas_mc, EarlyStopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8578436585823295"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_seas = load_model('..Models/seas_best_model.h5')\n",
    "\n",
    "y_predicted_seas = model_seas.predict(X_test)\n",
    "roc_auc_score(y_test, y_predicted_seas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9391003174160918"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full Data Test - Random Forest Network\n",
    "X_test = true_features\n",
    "y_test = true_labels_rf.to_numpy()\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test).reshape(-1,1)\n",
    "y_predicted_seas = model_seas.predict(X_test).reshape(-1,1)\n",
    "\n",
    "y_predicted = np.concatenate((y_predicted_h1n1, y_predicted_seas), axis=1)\n",
    "roc_auc_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 2)"
      ]
     },
     "execution_count": 701,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 2)"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32140653],\n",
       "       [0.4491121 ],\n",
       "       [0.28784873],\n",
       "       ...,\n",
       "       [0.44708622],\n",
       "       [0.35714154],\n",
       "       [0.13289283]])"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted_h1n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.321407</td>\n",
       "      <td>0.254216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.449112</td>\n",
       "      <td>0.241889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.287849</td>\n",
       "      <td>0.524687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.528227</td>\n",
       "      <td>0.277097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237950</td>\n",
       "      <td>0.188313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>0.429888</td>\n",
       "      <td>0.621552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>0.310791</td>\n",
       "      <td>0.758748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>0.447086</td>\n",
       "      <td>0.199280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>0.357142</td>\n",
       "      <td>0.575851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>0.132893</td>\n",
       "      <td>0.172749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "0     0.321407  0.254216\n",
       "1     0.449112  0.241889\n",
       "2     0.287849  0.524687\n",
       "3     0.528227  0.277097\n",
       "4     0.237950  0.188313\n",
       "...        ...       ...\n",
       "1495  0.429888  0.621552\n",
       "1496  0.310791  0.758748\n",
       "1497  0.447086  0.199280\n",
       "1498  0.357142  0.575851\n",
       "1499  0.132893  0.172749\n",
       "\n",
       "[1500 rows x 2 columns]"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8755682333879505"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full Data Test - Neural Network\n",
    "X_test = true_features\n",
    "y_test = true_labels\n",
    "\n",
    "y_predicted_h1n1 = model_h1n1.predict(X_test)[:,1].reshape(-1,1)\n",
    "y_predicted_seas = model_seas.predict(X_test)[:,1].reshape(-1,1)\n",
    "\n",
    "y_predicted = np.concatenate((y_predicted_h1n1, y_predicted_seas), axis=1)\n",
    "roc_auc_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../Data/test_set_features.csv')\n",
    "full_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = list(test.select_dtypes('number').columns)\n",
    "\n",
    "cat_cols = [\n",
    "    'race',\n",
    "    'sex',\n",
    "    'marital_status',\n",
    "    'rent_or_own',\n",
    "    'hhs_geo_region',\n",
    "    'census_msa',\n",
    "    'employment_industry',\n",
    "    'employment_occupation'\n",
    "]\n",
    "\n",
    "ord_cols = [\n",
    "    'age_group',\n",
    "    'education',\n",
    "    'income_poverty',\n",
    "    'employment_status'\n",
    "]\n",
    "\n",
    "\n",
    "#Impute Test\n",
    "for col in num_cols:\n",
    "    test[col] = test[col].fillna(value=-1)\n",
    "\n",
    "\n",
    "for col in (cat_cols+ord_cols):\n",
    "    test[col] = test[col].fillna(value='None')\n",
    "\n",
    "    \n",
    "test['age_group'] = test['age_group'].map({\n",
    "    '18 - 34 Years': 1,\n",
    "    '35 - 44 Years': 2,\n",
    "    '45 - 54 Years': 3,\n",
    "    '55 - 64 Years': 4,\n",
    "    '65+ Years': 5\n",
    "})\n",
    "    \n",
    "test['education'] = test['education'].map({\n",
    "    '< 12 Years': 1,\n",
    "    '12 Years': 2,\n",
    "    'Some College': 3,\n",
    "    'College Graduate': 4,\n",
    "    'None': -1\n",
    "})\n",
    "\n",
    "test['income_poverty'] = test['income_poverty'].map({\n",
    "    'None': -1,\n",
    "    'Below Poverty': 1,\n",
    "    '<= $75,000, Above Poverty': 2,\n",
    "    '> $75,000': 3\n",
    "})\n",
    "\n",
    "test['employment_status'] = test['employment_status'].map({\n",
    "    'None': -1,\n",
    "    'Unemployed': 1,\n",
    "    'Employed': 2,\n",
    "    'Not in Labor Force': 3\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[used_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ct.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_h1n1 = model_h1n1.predict(test)[:,1].reshape(-1,1)\n",
    "y_seas = model_seas.predict(test)[:,1].reshape(-1,1)\n",
    "\n",
    "y_comb = np.concatenate((y_h1n1, y_seas), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(y_comb, columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "\n",
    "submission = pd.concat([full_test, results], axis=1)\n",
    "submission = submission[['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().date()\n",
    "\n",
    "submission.to_csv(f'../Submissions/Neural Network Submission {today}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "      <th>seasonal_vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26707</td>\n",
       "      <td>0.427758</td>\n",
       "      <td>0.140502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26708</td>\n",
       "      <td>0.061124</td>\n",
       "      <td>0.008095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26709</td>\n",
       "      <td>0.766460</td>\n",
       "      <td>0.941280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26710</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.995322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26711</td>\n",
       "      <td>0.795208</td>\n",
       "      <td>0.761075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26703</th>\n",
       "      <td>53410</td>\n",
       "      <td>0.834162</td>\n",
       "      <td>0.635043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>53411</td>\n",
       "      <td>0.304972</td>\n",
       "      <td>0.187355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>53412</td>\n",
       "      <td>0.624410</td>\n",
       "      <td>0.066920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>53413</td>\n",
       "      <td>0.049624</td>\n",
       "      <td>0.339369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>53414</td>\n",
       "      <td>0.991245</td>\n",
       "      <td>0.736265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26708 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       respondent_id  h1n1_vaccine  seasonal_vaccine\n",
       "0              26707      0.427758          0.140502\n",
       "1              26708      0.061124          0.008095\n",
       "2              26709      0.766460          0.941280\n",
       "3              26710      0.988506          0.995322\n",
       "4              26711      0.795208          0.761075\n",
       "...              ...           ...               ...\n",
       "26703          53410      0.834162          0.635043\n",
       "26704          53411      0.304972          0.187355\n",
       "26705          53412      0.624410          0.066920\n",
       "26706          53413      0.049624          0.339369\n",
       "26707          53414      0.991245          0.736265\n",
       "\n",
       "[26708 rows x 3 columns]"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
